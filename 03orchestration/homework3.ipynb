{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Converting the script to a Prefect flow"
      ],
      "metadata": {
        "id": "BqwFtCUZe7V7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@flow(task_runner=SequentialTaskRunner())\n",
        "def main(train_path: str = 'data/fhv_tripdata_2021-01.parquet', val_path: str = 'data/fhv_tripdata_2021-02.parquet'):\n",
        "\n",
        "    categorical = ['PUlocationID', 'DOlocationID']\n",
        "\n",
        "    df_train = read_data(train_path)\n",
        "    df_train_processed = prepare_features(df_train, categorical)\n",
        "\n",
        "    df_val = read_data(val_path)\n",
        "    df_val_processed = prepare_features(df_val, categorical, False)\n",
        "\n",
        "    # train the model\n",
        "    lr, dv = train_model(df_train_processed, categorical).result()\n",
        "    run_model(df_val_processed, categorical, dv, lr)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "klTuuk5Ne73L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After adding all of the decorators, there is actually one task that you will need to call .result() for inside the flow to get it to work.\n",
        "\n",
        "Which task is this?"
      ],
      "metadata": {
        "id": "QRU7hWK_e7O0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A1 = 'train_model'\n",
        "print(f'Answer: {A1}')"
      ],
      "metadata": {
        "id": "dePRoPtcfE3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: Parameterizing the flow\n",
        "Where get_paths is a task that you have to implement. The specs for this are outlined in the motivation section. Listing them out again here:\n",
        "\n",
        "The flow will take in a parameter called date which will be a datetime. a. date should default to None b. If date is None, use the current day. Use the data from 2 months back as the training data and the data from the previous month as validation data. c. If a date value is supplied, get 2 months before the date as the training data, and the previous month as validation data. d. As a concrete example, if the date passed is \"2021-03-15\", the training data should be \"fhv_tripdata_2021-01.parquet\" and the validation file will be \"fhv_trip_data_2021-02.parquet\"\n",
        "\n",
        "The solution:"
      ],
      "metadata": {
        "id": "Q8U8fskje7LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@task\n",
        "def get_paths(date):\n",
        "    logger = get_run_logger()\n",
        "\n",
        "    if date == None:\n",
        "        date = datetime.date.today()\n",
        "    else:\n",
        "        date = pd.to_datetime(date)\n",
        "\n",
        "    date_train = date - relativedelta(months=2)\n",
        "    date_val = date - relativedelta(months=1)\n",
        "\n",
        "    date_train = date_train.strftime(\"%Y-%m\")\n",
        "    date_val = date_val.strftime(\"%Y-%m\")\n",
        "\n",
        "    # Complete val and train path\n",
        "    train_path = 'data/fhv_tripdata_'+date_train+'.parquet'\n",
        "    val_path = 'data/fhv_tripdata_'+date_val+'.parquet'  \n",
        "\n",
        "    if not os.path.exists(train_path):\n",
        "        url = f\"https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_{date_train}.parquet\"\n",
        "        os.system(f\"wget {url} -P \\data\")\n",
        "\n",
        "    if not os.path.exists(val_path):\n",
        "        url = f\"https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_{date_val}.parquet\"\n",
        "        os.system(f\"wget {url} -P \\data\")    \n",
        "\n",
        "\n",
        "    logger.info(f\"Train path: {train_path}\")\n",
        "    logger.info(f\"Val path: {val_path}\")\n",
        "\n",
        "    return train_path, val_path"
      ],
      "metadata": {
        "id": "fkvCfPIqfL5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This solution, will get the date input and return the paths that will be used for training and validation. This is defined by the given date. The training data is 2 months and the validation 1 months prior the given date."
      ],
      "metadata": {
        "id": "ppoo2P1LfNbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! python homework --date \"2021-03-15\""
      ],
      "metadata": {
        "id": "Fpbtxb87fUqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By setting up the logger to \"2021-08-15\", we should see some logs about our training job.\n",
        "What is the validation MSE when running the flow with this date?"
      ],
      "metadata": {
        "id": "AtumA7G6e7F7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d_gmrbnd2u6"
      },
      "outputs": [],
      "source": [
        "A2 = 'The MSE of validation is: 11.637032331753268'\n",
        "print(f'Answer: {A2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: Saving the model and artifacts\n",
        "Save the model as \"model-{date}.pkl\" where date is in YYYY-MM-DD. Note that date here is the value of the flow parameter. In practice, this setup makes it very easy to get the latest model to run predictions because you just need to get the most recent one.\n",
        "In this example we use a DictVectorizer. That is needed to run future data through our model. Save that as \"dv-{date}.pkl\". Similar to above, if the date is 2021-03-15, the files output should be model-2021-03-15.bin and dv-2021-03-15.b.\n",
        "What is the file size of the DictVectorizer that we trained when the date is 2021-08-15?"
      ],
      "metadata": {
        "id": "bFcpHC-qfge7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -la models"
      ],
      "metadata": {
        "id": "gBY0QfDafk_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A3 = 13191\n",
        "print(f'Answer: {A3}')"
      ],
      "metadata": {
        "id": "EjKka8SZfp4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4: Creating a deployment with a CronSchedule\n",
        "We previously showed the IntervalSchedule in the video tutorials. In some cases, the interval is too rigid. For example, what if we wanted to run this flow on the 15th of every month? An interval of 30 days would not be in sync. In cases like these, the CronSchedule is more appropriate. The documentation for that is here\n",
        "\n",
        "What is the Cron expression to run a flow at 9 AM every 15th of the month?\n",
        "One can use this [link](https://crontab.guru/#0_9_15_*_*) to get the right code to feed to cron function"
      ],
      "metadata": {
        "id": "Nn6d58YrfgSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@flow(task_runner=SequentialTaskRunner())\n",
        "def main(date=\"2021-08-15\"):\n",
        "\n",
        "    train_path, val_path = get_paths(date).result()\n",
        "\n",
        "    categorical = ['PUlocationID', 'DOlocationID']\n",
        "\n",
        "    df_train = read_data(train_path)\n",
        "    df_train_processed = prepare_features(df_train, categorical)\n",
        "\n",
        "    df_val = read_data(val_path)\n",
        "    df_val_processed = prepare_features(df_val, categorical, False)\n",
        "\n",
        "    # train the model\n",
        "    lr, dv = train_model(df_train_processed, categorical).result()\n",
        "    run_model(df_val_processed, categorical, dv, lr)\n",
        "\n",
        "    # Save models\n",
        "    with open(f\"models/model-{date}.bin\", 'wb') as f_out:\n",
        "        pickle.dump(lr, f_out)\n",
        "\n",
        "    with open(f\"models/dv-{date}.bin\", 'wb') as f_out:\n",
        "        pickle.dump(dv, f_out)\n",
        "\n",
        "\n",
        "from prefect.deployments import DeploymentSpec\n",
        "from prefect.orion.schemas.schedules import CronSchedule\n",
        "from prefect.flow_runners import SubprocessFlowRunner\n",
        "\n",
        "DeploymentSpec(\n",
        "    flow=main,\n",
        "    name='model_training',\n",
        "    schedule=CronSchedule(cron='0 9 15 * *'), #We can set a timezone, by default uses your computer time\n",
        "    flow_runner=SubprocessFlowRunner(),\n",
        "    tags=['first_model'],\n",
        ")"
      ],
      "metadata": {
        "id": "kU9bIlUxfv9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A4 = '0 9 15 * *'\n",
        "print(f'Answer: {A4}')"
      ],
      "metadata": {
        "id": "BXRtZAWrf4HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5: Viewing the Deployment\n",
        "View the deployment in the UI. When first loading, we may not see that many flows because the default filter is 1 day back and 1 day forward. Remove the filter for 1 day forward to see the scheduled runs.\n",
        "\n",
        "How many flow runs are scheduled by Prefect in advanced? You should not be counting manually. There is a number of upcoming runs on the top right of the dashboard."
      ],
      "metadata": {
        "id": "u_OlirZlfgNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A5 = 4\n",
        "print(f'Answer: {A5}')"
      ],
      "metadata": {
        "id": "hYZOslzagbOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6: Creating a work-queue\n",
        "In order to run this flow, you will need an agent and a work queue (Run it by using the UI). Because we scheduled our flow or every month, it won't really get picked up by an agent. For this exercise, create a work-queue from the UI and view it using the CLI.\n",
        "\n",
        "For all CLI commands with Prefect, you can use --help to get more information.\n",
        "\n",
        "For example,\n",
        "\n",
        "prefect --help\n",
        "prefect work-queue --help\n",
        "What is the command to view the available work-queues?"
      ],
      "metadata": {
        "id": "hMRpEyOQfgBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! prefect work-queue ls"
      ],
      "metadata": {
        "id": "qD2roX7TgrIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A6 = 'prefect work-queue ls'\n",
        "print(f'Answer: {A6}')"
      ],
      "metadata": {
        "id": "DXluezVcguIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from prefect import flow, task, get_run_logger\n",
        "from prefect.task_runners import SequentialTaskRunner\n",
        "\n",
        "\n",
        "@task\n",
        "def read_data(path):\n",
        "    df = pd.read_parquet(path)\n",
        "    return df\n",
        "\n",
        "@task\n",
        "def prepare_features(df, categorical, train=True):\n",
        "\n",
        "    logger = get_run_logger()\n",
        "    df['duration'] = df.dropOff_datetime - df.pickup_datetime\n",
        "    df['duration'] = df.duration.dt.total_seconds() / 60\n",
        "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
        "\n",
        "    mean_duration = df.duration.mean()\n",
        "    if train:\n",
        "        logger.info(f\"The mean duration of training is {mean_duration}\")\n",
        "    else:\n",
        "        logger.info(f\"The mean duration of validation is {mean_duration}\")\n",
        "\n",
        "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
        "    return df\n",
        "\n",
        "@task\n",
        "def train_model(df, categorical):\n",
        "    logger = get_run_logger()\n",
        "\n",
        "    train_dicts = df[categorical].to_dict(orient='records')\n",
        "    dv = DictVectorizer()\n",
        "    X_train = dv.fit_transform(train_dicts) \n",
        "    y_train = df.duration.values\n",
        "\n",
        "    logger.info(f\"The shape of X_train is {X_train.shape}\")\n",
        "    logger.info(f\"The DictVectorizer has {len(dv.feature_names_)} features\")\n",
        "\n",
        "    lr = LinearRegression()\n",
        "    lr.fit(X_train, y_train)\n",
        "    y_pred = lr.predict(X_train)\n",
        "    mse = mean_squared_error(y_train, y_pred, squared=False)\n",
        "    logger.info(f\"The MSE of training is: {mse}\")\n",
        "    return lr, dv\n",
        "\n",
        "@task\n",
        "def run_model(df, categorical, dv, lr):\n",
        "    logger = get_run_logger()\n",
        "    val_dicts = df[categorical].to_dict(orient='records')\n",
        "    X_val = dv.transform(val_dicts) \n",
        "    y_pred = lr.predict(X_val)\n",
        "    y_val = df.duration.values\n",
        "\n",
        "    mse = mean_squared_error(y_val, y_pred, squared=False)\n",
        "    logger.info(f\"The MSE of validation is: {mse}\")\n",
        "    return\n",
        "\n",
        "@task\n",
        "def get_paths(date):\n",
        "    logger = get_run_logger()\n",
        "\n",
        "    if date == None:\n",
        "        date = datetime.date.today()\n",
        "    else:\n",
        "        date = pd.to_datetime(date)\n",
        "\n",
        "    date_train = date - relativedelta(months=2)\n",
        "    date_val = date - relativedelta(months=1)\n",
        "\n",
        "    date_train = date_train.strftime(\"%Y-%m\")\n",
        "    date_val = date_val.strftime(\"%Y-%m\")\n",
        "\n",
        "    # Complete val and train path\n",
        "    train_path = 'data/fhv_tripdata_'+date_train+'.parquet'\n",
        "    val_path = 'data/fhv_tripdata_'+date_val+'.parquet'  \n",
        "\n",
        "    if not os.path.exists(train_path):\n",
        "        url = f\"https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_{date_train}.parquet\"\n",
        "        try:\n",
        "            os.system(f\"wget {url} -P \\data\")\n",
        "        except:\n",
        "            logger.warning(f\"{url} not available\")\n",
        "            raise Exception('Train data not available')\n",
        "\n",
        "    if not os.path.exists(val_path):\n",
        "        url = f\"https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_{date_val}.parquet\"\n",
        "        try:\n",
        "            os.system(f\"wget {url} -P \\data\")\n",
        "        except:\n",
        "            logger.warning(f\"{url} not available\")\n",
        "            raise Exception('Val data not available')\n",
        "\n",
        "\n",
        "\n",
        "    logger.info(f\"Train path: {train_path}\")\n",
        "    logger.info(f\"Val path: {val_path}\")\n",
        "\n",
        "    return train_path, val_path\n",
        "\n",
        "\n",
        "@flow(task_runner=SequentialTaskRunner())\n",
        "def main(date=\"2021-08-15\"):\n",
        "\n",
        "    train_path, val_path = get_paths(date).result()\n",
        "\n",
        "    categorical = ['PUlocationID', 'DOlocationID']\n",
        "\n",
        "    df_train = read_data(train_path)\n",
        "    df_train_processed = prepare_features(df_train, categorical)\n",
        "\n",
        "    df_val = read_data(val_path)\n",
        "    df_val_processed = prepare_features(df_val, categorical, False)\n",
        "\n",
        "    # train the model\n",
        "    lr, dv = train_model(df_train_processed, categorical).result()\n",
        "    run_model(df_val_processed, categorical, dv, lr)\n",
        "    \n",
        "    # Save models\n",
        "    with open(f\"models/model-{date}.bin\", 'wb') as f_out:\n",
        "        pickle.dump(lr, f_out)\n",
        "\n",
        "    with open(f\"models/dv-{date}.bin\", 'wb') as f_out:\n",
        "        pickle.dump(dv, f_out)\n",
        "\n",
        "\n",
        "from prefect.deployments import DeploymentSpec\n",
        "from prefect.orion.schemas.schedules import CronSchedule\n",
        "from prefect.flow_runners import SubprocessFlowRunner\n",
        "\n",
        "DeploymentSpec(\n",
        "    flow=main,\n",
        "    name='model_training',\n",
        "    schedule=CronSchedule(cron='0 9 15 * *'),\n",
        "    flow_runner=SubprocessFlowRunner(),\n",
        "    tags=['first_model'],\n",
        ")\n",
        "\n",
        "\n",
        "# For Question 1/2/3\n",
        "#if __name__ == '__main__':\n",
        "#\n",
        "#   parser = argparse.ArgumentParser()\n",
        "#    parser.add_argument(\n",
        "#        \"--date\",\n",
        "#        default=None,\n",
        "#        help=\"Date to catch the data to train and validate\"\n",
        "#    )\n",
        "#    args = parser.parse_args()\n",
        "#\n",
        "#    main(args.date)\n"
      ],
      "metadata": {
        "id": "-T_X1dmUgvG7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}